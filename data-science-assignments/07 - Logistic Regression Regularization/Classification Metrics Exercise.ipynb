{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification Metrics Exercise.ipynb","provenance":[{"file_id":"1Yopa76biOpr-fRCIEq71KLrslvLes-Ej","timestamp":1638605936233}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QIBuSj1RW_JB"},"source":["# Classification Metrics:\n","\n","\n","![target image](https://github.com/ninja-josh/image-storage/raw/main/qft5tas90c801%20(1).jpeg)\n","\n","## How do we know if our model is any good?\n"]},{"cell_type":"markdown","metadata":{"id":"X2edK4bNx-5Y"},"source":["## Regression vs Classification Metrics\n","\n","### Regression Metrics\n","\n","In a regression model a target label could have any value (theoretically).\n","\n","When we are creating a regression model, we try to create a model that predicts a label that is as close to the true label value for a sample as possible.  This is why we use metrics like mean absolute error, mean squared error, or root mean squared error.  We want to know how far away the prediction from the truth.  In fact, our model may never make a perfectly accurate prediction and that's fine, as long as it is close enough.\n","\n","### Classification Metrics\n","\n","With classification models each sample is a member of one of a finite number of classes.  For each sample, either the model predicts the correct class or predicts one of the incorrect classes.  It's right or wrong, there is no 'close'.\n","\n","Because of this we need different metrics.  In this lesson we will explore how to evaluate a classification model using:\n","\n","1. Accuracy\n","2. Precision\n","3. Recall\n","4. A Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"8MEED0E4ZHJO","executionInfo":{"status":"ok","timestamp":1638738272755,"user_tz":360,"elapsed":1284,"user":{"displayName":"Michael Akinosho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1RaAyEpJQ07m9M7YAeaMlYgbOT5eAtJmP70YX=s64","userId":"02347695828138672204"}}},"source":["import pandas as pd\n","import numpy as np\n","#import seaborn to make a nice heatmap for our confusion matrix\n","import seaborn as sns\n","\n","#import some necessary tools\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","#import accuracy, precision, recall, classification report, and confusion matrix scoring functions\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n","\n","#Importing the KNN Classifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.dummy import DummyClassifier"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghZG2YZV2lLW"},"source":["'Breast Cancer Wisconsin' is a binary classification dataset that comes with the sklearn package in order to demonstrate and experiment with models.  We will use this well studied and pre-cleaned dataset to demonstrate how to evaluate a classification model on a binary classification problem.  Each record in this dataset is a mass in a breast and each feature is a measurement of that mass.  The target is 0 = benign, or 1 = malignant.\n","\n","Our task will be to create a model that classifies a given mass as either benign or malignant.  "]},{"cell_type":"code","metadata":{"id":"zbwkH-kG2XU6","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"ok","timestamp":1638738276674,"user_tz":360,"elapsed":144,"user":{"displayName":"Michael Akinosho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1RaAyEpJQ07m9M7YAeaMlYgbOT5eAtJmP70YX=s64","userId":"02347695828138672204"}},"outputId":"6582f81e-1cd3-4c53-9f89-39ff3ec0417c"},"source":["#Load the Data\n","data = load_breast_cancer()\n","X = pd.DataFrame(data.data, columns = data.feature_names)\n","y = pd.DataFrame(data.target, columns=['outcome'])\n","print(y.value_counts(normalize=True))\n","X.head()"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["outcome\n","1          0.627417\n","0          0.372583\n","dtype: float64\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean radius</th>\n","      <th>mean texture</th>\n","      <th>mean perimeter</th>\n","      <th>mean area</th>\n","      <th>mean smoothness</th>\n","      <th>mean compactness</th>\n","      <th>mean concavity</th>\n","      <th>mean concave points</th>\n","      <th>mean symmetry</th>\n","      <th>mean fractal dimension</th>\n","      <th>radius error</th>\n","      <th>texture error</th>\n","      <th>perimeter error</th>\n","      <th>area error</th>\n","      <th>smoothness error</th>\n","      <th>compactness error</th>\n","      <th>concavity error</th>\n","      <th>concave points error</th>\n","      <th>symmetry error</th>\n","      <th>fractal dimension error</th>\n","      <th>worst radius</th>\n","      <th>worst texture</th>\n","      <th>worst perimeter</th>\n","      <th>worst area</th>\n","      <th>worst smoothness</th>\n","      <th>worst compactness</th>\n","      <th>worst concavity</th>\n","      <th>worst concave points</th>\n","      <th>worst symmetry</th>\n","      <th>worst fractal dimension</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.3001</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>0.07871</td>\n","      <td>1.0950</td>\n","      <td>0.9053</td>\n","      <td>8.589</td>\n","      <td>153.40</td>\n","      <td>0.006399</td>\n","      <td>0.04904</td>\n","      <td>0.05373</td>\n","      <td>0.01587</td>\n","      <td>0.03003</td>\n","      <td>0.006193</td>\n","      <td>25.38</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.0869</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>0.05667</td>\n","      <td>0.5435</td>\n","      <td>0.7339</td>\n","      <td>3.398</td>\n","      <td>74.08</td>\n","      <td>0.005225</td>\n","      <td>0.01308</td>\n","      <td>0.01860</td>\n","      <td>0.01340</td>\n","      <td>0.01389</td>\n","      <td>0.003532</td>\n","      <td>24.99</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.1974</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>0.05999</td>\n","      <td>0.7456</td>\n","      <td>0.7869</td>\n","      <td>4.585</td>\n","      <td>94.03</td>\n","      <td>0.006150</td>\n","      <td>0.04006</td>\n","      <td>0.03832</td>\n","      <td>0.02058</td>\n","      <td>0.02250</td>\n","      <td>0.004571</td>\n","      <td>23.57</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.2414</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>0.09744</td>\n","      <td>0.4956</td>\n","      <td>1.1560</td>\n","      <td>3.445</td>\n","      <td>27.23</td>\n","      <td>0.009110</td>\n","      <td>0.07458</td>\n","      <td>0.05661</td>\n","      <td>0.01867</td>\n","      <td>0.05963</td>\n","      <td>0.009208</td>\n","      <td>14.91</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.1980</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>0.05883</td>\n","      <td>0.7572</td>\n","      <td>0.7813</td>\n","      <td>5.438</td>\n","      <td>94.44</td>\n","      <td>0.011490</td>\n","      <td>0.02461</td>\n","      <td>0.05688</td>\n","      <td>0.01885</td>\n","      <td>0.01756</td>\n","      <td>0.005115</td>\n","      <td>22.54</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n","0        17.99         10.38  ...          0.4601                  0.11890\n","1        20.57         17.77  ...          0.2750                  0.08902\n","2        19.69         21.25  ...          0.3613                  0.08758\n","3        11.42         20.38  ...          0.6638                  0.17300\n","4        20.29         14.34  ...          0.2364                  0.07678\n","\n","[5 rows x 30 columns]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"s5KB8Y5i2gS9","executionInfo":{"status":"ok","timestamp":1638738658170,"user_tz":360,"elapsed":102,"user":{"displayName":"Michael Akinosho","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1RaAyEpJQ07m9M7YAeaMlYgbOT5eAtJmP70YX=s64","userId":"02347695828138672204"}}},"source":["#Prepare the data for modeling and validation\n","#Train-test split.  Set the random state to 42\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVPyRoxE7xdd"},"source":["# Baseline vs KNN"]},{"cell_type":"code","metadata":{"id":"Qg6ogWgM7W3m"},"source":["#Create a pipeline with a StandardScaler and a KNeighborsClassifier\n","#Create another pipeline with StandardScaler and DummyClassifier\n","#using the 'most_frequent' strategy\n","\n","#Fit both model pipelines and save their predictions test sets\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEr-cVH9O0ND"},"source":["Quickly remind yourself of what you learned about type 1 and type 2 errors.  In every binary classification problem we select one class to be the **'positive'** class and one to be the **'negative'** class.  The positive class should be the one you are most interested in finding.  For our breast cancer dataset the positive class will be the malignant masses and the negative class will be the benign one.\n","\n","## Type 1 error:\n","If our model predicts that a mass is malignant, but it is in fact benign, it will have made a type 1 error.  This is also known as a false positive\n","\n","## Type 2 error:\n","If our model predicts that a mass is benign, when in fact it is malignant, it will have made a type 2 error.  This is is also known as a false negative.\n","\n","\n","*Which of these do you think is worse in this case?  If we have to increase one kind of error in order to minimize the other kind, which would we want to minimize?  Why?*"]},{"cell_type":"markdown","metadata":{"id":"TJ--mFdZN21E"},"source":["# Accuracy\n","\n","Accuracy is the metric that is most intuitive.  This is defined as:\n","\n","$$\n","accuracy = \\frac{True  Positives + True  Negatives}{All  Samples}\n","$$\n","\n","In other words accuracy is correct predictions our model made out of the total number of predictions.\n","\n","Pros:\n","Accuracy is easy to understand and gives a combined picture of both kinds of errors in one number.\n","\n","Cons: Accuracy can be deceiving when a dataset is unbalanced.  It also does not give specific information about the kinds of errors that a model is making.\n","\n","For example, we saw above that 62% of our samples are malign masses when we did `y.value_counts(normalize=True)`"]},{"cell_type":"markdown","metadata":{"id":"T3p1w80pNXKM"},"source":["To use the sklearn metrics functions we pass them first the true labels, then the predicted labels.  For example: `accuracy = accuracy_score(y_test, y_pred)`"]},{"cell_type":"code","metadata":{"id":"6iAa_70TNw_8"},"source":["#Print the accuracy of both models on the test set\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2c7ChhLV2cT"},"source":["If our dataset were even more imbalanced, say 99.9% malignant, then a prediction that EVERYTHING is malignant would have a very high accuracy.  However, that would not be a very useful model for actual medical use.  More often we see the opposite: a disease is very rare, occurring .01% of the time or less, and a model that predicts that NO samples ever have the disease will have a high accuracy, but will actually be useless...and dangerous!"]},{"cell_type":"markdown","metadata":{"id":"sKCBKNtvUQae"},"source":["# Recall\n","\n","When we want to reduce the number of false negatives, we want to improve recall.\n","\n","Recall is defined as: \n","\n","$$\n","recall = \\frac{True Positives}{False Negatives + True Positives}\n","$$\n","\n","That is to say: how many samples did our model label as positive out of all of the true positive samples?\n","\n","Pros: A higher recall means a fewer false negative predictions, also known as type 2 errors.  It's great for when classifying a positive as a negative is a costly mistake.\n","\n","Cons: Does not consider how many samples are falsely labeled as positive, or false positives.  It does not penalize type 1 errors.\n","\n","In the case of this dataset, we might assume that the consequence for a false negative is that a person needlessly dies from cancer while the consequence for a false positive is that someone has unnecessary surgery.  While neither is great, the second is generally going to be less bad.  A high recall means fewer malignant masses going untreated."]},{"cell_type":"code","metadata":{"id":"BwboFujvYHSs"},"source":["#Print the recall scores of both models.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyM5xz6lYWTg"},"source":["\n","You can see that our KNN model has a high recall, but just predicting that ALL tumors are malignant gives us a perfect recall of 1!  While we want to catch as many malignant tumors as possible, we don't want to just send everyone under the knife, especially since we know that 38% don't need surgery!\n","\n","# Precision\n","\n","When we want to reduce the number of false positives, we want to improve precision.\n","\n","Precision is defined as:\n","\n","$$\n","precision = \\frac{True Positives}{False Positives + True Positives}\n","$$\n","\n","In other words: What ratio of the samples that we predicted were in the positive class were actually in the positive class?\n","\n","Pros:  A high precision means fewer type 1 errors, or fewer false positives.  This is a good metric to maximize if a false positive prediction is a costly mistake.\n","\n","Cons: Precision does not penalize a model for false negatives.  It does not count type 2 errors.\n","\n","In this case precision would be measuring how many of the tumors we elected to operate on were actually malignant."]},{"cell_type":"code","metadata":{"id":"sjuf4fvJajS-"},"source":["#Print the precision scores of both models.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OOB0phAWbCSQ"},"source":["# The Complete Picture: Confusion Matrices and classification_report()\n","\n","As you have seen, precision, precision, and recall each only tell part of the story.  In order to get the full picture of how your model is performing and what kinds of mistakes it tends to make, you need to look at a confusion matrix and/or sklearn's handy `classification_report()` function."]},{"cell_type":"code","metadata":{"id":"6xUmwY-qa8DL"},"source":["#Use confusion_matrix() to print confusion matrices for both models\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YXAJ2jpZbt0i"},"source":["The confusion matrix above is informational, but rather dry.  I prefer to use seaborns `heatmap()` function to liven it up a little"]},{"cell_type":"code","metadata":{"id":"7hAfxnzCbse5"},"source":["#Use sns.heatmap() to plot the confusion matrices from above.  \n","#Set annot = True to see the numbers, and change the colormap to something easier to read\n","#with cmap='Greens' (or whatever color map you like)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M1nZ-FEKcIfx"},"source":["We see the false positives predictions (top right) and false negative predictions (bottom left) that our model made.  However, the bottom left is out of 89 total and the top right is out of 54 total.  We can see the normalized ratios of true and false predictions by normalizing along the 'true' or 'pred' axes in the confusion_matrix() function.  \n","\n","To normalize along the 'true' axis, we set `confusion_matrix(y_test, y_prediction, normalize = 'true')`.  \n","\n","Notice that it is the string 'true' NOT the boolean value: `True`"]},{"cell_type":"code","metadata":{"id":"8B20cEmYb4oB"},"source":["#Recreate the confusion matrix above, but with the values normalized along the 'true' axis.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pt2VjHsfc5pY"},"source":["We can now see that our KNN model successfully classified 99% of the samples in our test set that were malignant, class 1, and 89% of the samples in our test set that were benign, class 0.\n","\n","Another quick way to check the accuracy, recall, and precision of a model on a test set is with `classification_report()`, which runs several metrics on both classes simultaneously."]},{"cell_type":"code","metadata":{"id":"Jz0bpAN3cqmw","collapsed":true},"source":["#Use classifiction_report() to print a report of several metrics for all classes at once\n","#for both models\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hnz0EbE6dbPR"},"source":["You'll notice that each class has a different recall and precision.  f1-score, by the way, is the harmonic mean of the precision and recall.  \n","\n","classification_report also tells us the averages of the precisions, means, and f1-scores.  'support' is how many samples there are of each class."]},{"cell_type":"markdown","metadata":{"id":"W0GZG210h5y7"},"source":["# Multi-class Metrics\n","\n","Precision, recall, and accuracy also extend to cases when we have more than 2 possible classes.  However, in order to know how to calculate precision and recall we have to decide which class is our positive class.  We also become interested in the most complex patterns of errors that can occur.  \n","\n","For examples, suppose we have a problem with 3 possbile classes.  We might ask: \n","\n","When our model misclassifies class 1 samples, is it more often classifying them as class 0, or class 2?  This my help us understand why it is making that kind of mistake and how we can improve performance.\n","\n","This is where confusion matrices become even more useful in understanding how our model is behaving."]},{"cell_type":"markdown","metadata":{"id":"H3pCVHISdXXb"},"source":["# Summary\n","\n","Accuracy, precision, and recall all are metrics that give us different insights into how our model is performing in making predictions.  No one of them alone tells us everything, and different metrics are more or less important depending on our business problem.\n","\n","However, we need to measure all three to make sure our model is making useful predictions.  Two ways to do this quickly are with a confusion matrix and a classification report."]}]}